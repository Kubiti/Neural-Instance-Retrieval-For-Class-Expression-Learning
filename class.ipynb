{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QmGSDwkgfBKs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Expression          Individual       Example Type\n",
      "0         Cat       animals#cat01  positive examples\n",
      "1         Cat       animals#boy01  negative examples\n",
      "2         Cat    animals#lizard01  negative examples\n",
      "3         Cat     animals#snake01  negative examples\n",
      "4         Cat     animals#trout01  negative examples\n",
      "5         Cat       animals#bat01  negative examples\n",
      "6         Cat     animals#eagle01  negative examples\n",
      "7         Cat      animals#trex01  negative examples\n",
      "8         Cat   animals#ostrich01  negative examples\n",
      "9         Cat       animals#eel01  negative examples\n",
      "10        Cat       animals#dog01  negative examples\n",
      "11        Cat    animals#dragon01  negative examples\n",
      "12        Cat      animals#girl01  negative examples\n",
      "13        Cat    animals#turtle01  negative examples\n",
      "14        Cat     animals#shark01  negative examples\n",
      "15        Cat   animals#dolphin01  negative examples\n",
      "16        Cat   animals#herring01  negative examples\n",
      "17        Cat     animals#croco01  negative examples\n",
      "18        Cat  animals#platypus01  negative examples\n",
      "19        Cat   animals#penguin01  negative examples\n",
      "['positive examples' 'negative examples']\n",
      "(205976, 3)\n"
     ]
    }
   ],
   "source": [
    "%run data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_ind</th>\n",
       "      <th>1_ind</th>\n",
       "      <th>2_ind</th>\n",
       "      <th>3_ind</th>\n",
       "      <th>4_ind</th>\n",
       "      <th>5_ind</th>\n",
       "      <th>6_ind</th>\n",
       "      <th>7_ind</th>\n",
       "      <th>8_ind</th>\n",
       "      <th>9_ind</th>\n",
       "      <th>...</th>\n",
       "      <th>30_cla</th>\n",
       "      <th>31_cla</th>\n",
       "      <th>32_cla</th>\n",
       "      <th>33_cla</th>\n",
       "      <th>34_cla</th>\n",
       "      <th>35_cla</th>\n",
       "      <th>36_cla</th>\n",
       "      <th>37_cla</th>\n",
       "      <th>38_cla</th>\n",
       "      <th>39_cla</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.183570</td>\n",
       "      <td>0.064570</td>\n",
       "      <td>0.119314</td>\n",
       "      <td>-0.220391</td>\n",
       "      <td>0.185930</td>\n",
       "      <td>0.059797</td>\n",
       "      <td>0.320332</td>\n",
       "      <td>0.277950</td>\n",
       "      <td>-0.172449</td>\n",
       "      <td>-0.078276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190652</td>\n",
       "      <td>-0.317788</td>\n",
       "      <td>-0.006029</td>\n",
       "      <td>-0.285176</td>\n",
       "      <td>0.176120</td>\n",
       "      <td>-0.070169</td>\n",
       "      <td>-0.269090</td>\n",
       "      <td>-0.445585</td>\n",
       "      <td>-0.012556</td>\n",
       "      <td>0.332329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.326895</td>\n",
       "      <td>-0.017734</td>\n",
       "      <td>0.072276</td>\n",
       "      <td>-0.148582</td>\n",
       "      <td>0.154523</td>\n",
       "      <td>0.143247</td>\n",
       "      <td>0.258985</td>\n",
       "      <td>0.121672</td>\n",
       "      <td>-0.082587</td>\n",
       "      <td>0.207627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190652</td>\n",
       "      <td>-0.317788</td>\n",
       "      <td>-0.006029</td>\n",
       "      <td>-0.285176</td>\n",
       "      <td>0.176120</td>\n",
       "      <td>-0.070169</td>\n",
       "      <td>-0.269090</td>\n",
       "      <td>-0.445585</td>\n",
       "      <td>-0.012556</td>\n",
       "      <td>0.332329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.348107</td>\n",
       "      <td>-0.256041</td>\n",
       "      <td>0.186654</td>\n",
       "      <td>-0.198851</td>\n",
       "      <td>0.320173</td>\n",
       "      <td>0.172015</td>\n",
       "      <td>0.128844</td>\n",
       "      <td>0.057844</td>\n",
       "      <td>-0.132149</td>\n",
       "      <td>-0.147851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190652</td>\n",
       "      <td>-0.317788</td>\n",
       "      <td>-0.006029</td>\n",
       "      <td>-0.285176</td>\n",
       "      <td>0.176120</td>\n",
       "      <td>-0.070169</td>\n",
       "      <td>-0.269090</td>\n",
       "      <td>-0.445585</td>\n",
       "      <td>-0.012556</td>\n",
       "      <td>0.332329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.341165</td>\n",
       "      <td>0.044748</td>\n",
       "      <td>0.248957</td>\n",
       "      <td>-0.215969</td>\n",
       "      <td>0.217901</td>\n",
       "      <td>0.206946</td>\n",
       "      <td>0.298251</td>\n",
       "      <td>0.171259</td>\n",
       "      <td>-0.045913</td>\n",
       "      <td>-0.147986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190652</td>\n",
       "      <td>-0.317788</td>\n",
       "      <td>-0.006029</td>\n",
       "      <td>-0.285176</td>\n",
       "      <td>0.176120</td>\n",
       "      <td>-0.070169</td>\n",
       "      <td>-0.269090</td>\n",
       "      <td>-0.445585</td>\n",
       "      <td>-0.012556</td>\n",
       "      <td>0.332329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.115120</td>\n",
       "      <td>-0.028380</td>\n",
       "      <td>0.157293</td>\n",
       "      <td>-0.273941</td>\n",
       "      <td>0.275957</td>\n",
       "      <td>0.136321</td>\n",
       "      <td>0.229726</td>\n",
       "      <td>0.066404</td>\n",
       "      <td>-0.059985</td>\n",
       "      <td>0.030648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190652</td>\n",
       "      <td>-0.317788</td>\n",
       "      <td>-0.006029</td>\n",
       "      <td>-0.285176</td>\n",
       "      <td>0.176120</td>\n",
       "      <td>-0.070169</td>\n",
       "      <td>-0.269090</td>\n",
       "      <td>-0.445585</td>\n",
       "      <td>-0.012556</td>\n",
       "      <td>0.332329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8183</th>\n",
       "      <td>0.231076</td>\n",
       "      <td>-0.074579</td>\n",
       "      <td>0.161141</td>\n",
       "      <td>-0.239554</td>\n",
       "      <td>0.166832</td>\n",
       "      <td>0.142751</td>\n",
       "      <td>0.275325</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>-0.062771</td>\n",
       "      <td>-0.113664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.297430</td>\n",
       "      <td>0.169880</td>\n",
       "      <td>0.206478</td>\n",
       "      <td>-0.285225</td>\n",
       "      <td>-0.196411</td>\n",
       "      <td>-0.136789</td>\n",
       "      <td>-0.248808</td>\n",
       "      <td>-0.360837</td>\n",
       "      <td>-0.111585</td>\n",
       "      <td>0.350478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8184</th>\n",
       "      <td>0.183570</td>\n",
       "      <td>0.064570</td>\n",
       "      <td>0.119314</td>\n",
       "      <td>-0.220391</td>\n",
       "      <td>0.185930</td>\n",
       "      <td>0.059797</td>\n",
       "      <td>0.320332</td>\n",
       "      <td>0.277950</td>\n",
       "      <td>-0.172449</td>\n",
       "      <td>-0.078276</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.297430</td>\n",
       "      <td>0.169880</td>\n",
       "      <td>0.206478</td>\n",
       "      <td>-0.285225</td>\n",
       "      <td>-0.196411</td>\n",
       "      <td>-0.136789</td>\n",
       "      <td>-0.248808</td>\n",
       "      <td>-0.360837</td>\n",
       "      <td>-0.111585</td>\n",
       "      <td>0.350478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8185</th>\n",
       "      <td>0.265107</td>\n",
       "      <td>0.035628</td>\n",
       "      <td>0.237405</td>\n",
       "      <td>-0.208801</td>\n",
       "      <td>0.326406</td>\n",
       "      <td>0.151469</td>\n",
       "      <td>0.338959</td>\n",
       "      <td>0.051302</td>\n",
       "      <td>-0.042855</td>\n",
       "      <td>0.286972</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.297430</td>\n",
       "      <td>0.169880</td>\n",
       "      <td>0.206478</td>\n",
       "      <td>-0.285225</td>\n",
       "      <td>-0.196411</td>\n",
       "      <td>-0.136789</td>\n",
       "      <td>-0.248808</td>\n",
       "      <td>-0.360837</td>\n",
       "      <td>-0.111585</td>\n",
       "      <td>0.350478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8186</th>\n",
       "      <td>0.194329</td>\n",
       "      <td>-0.162931</td>\n",
       "      <td>0.175191</td>\n",
       "      <td>-0.272689</td>\n",
       "      <td>0.295109</td>\n",
       "      <td>0.161218</td>\n",
       "      <td>0.429787</td>\n",
       "      <td>0.174258</td>\n",
       "      <td>-0.076592</td>\n",
       "      <td>-0.058829</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.297430</td>\n",
       "      <td>0.169880</td>\n",
       "      <td>0.206478</td>\n",
       "      <td>-0.285225</td>\n",
       "      <td>-0.196411</td>\n",
       "      <td>-0.136789</td>\n",
       "      <td>-0.248808</td>\n",
       "      <td>-0.360837</td>\n",
       "      <td>-0.111585</td>\n",
       "      <td>0.350478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8187</th>\n",
       "      <td>0.341165</td>\n",
       "      <td>0.044748</td>\n",
       "      <td>0.248957</td>\n",
       "      <td>-0.215969</td>\n",
       "      <td>0.217901</td>\n",
       "      <td>0.206946</td>\n",
       "      <td>0.298251</td>\n",
       "      <td>0.171259</td>\n",
       "      <td>-0.045913</td>\n",
       "      <td>-0.147986</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.297430</td>\n",
       "      <td>0.169880</td>\n",
       "      <td>0.206478</td>\n",
       "      <td>-0.285225</td>\n",
       "      <td>-0.196411</td>\n",
       "      <td>-0.136789</td>\n",
       "      <td>-0.248808</td>\n",
       "      <td>-0.360837</td>\n",
       "      <td>-0.111585</td>\n",
       "      <td>0.350478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8188 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0_ind     1_ind     2_ind     3_ind     4_ind     5_ind     6_ind  \\\n",
       "0     0.183570  0.064570  0.119314 -0.220391  0.185930  0.059797  0.320332   \n",
       "1     0.326895 -0.017734  0.072276 -0.148582  0.154523  0.143247  0.258985   \n",
       "2     0.348107 -0.256041  0.186654 -0.198851  0.320173  0.172015  0.128844   \n",
       "3     0.341165  0.044748  0.248957 -0.215969  0.217901  0.206946  0.298251   \n",
       "4     0.115120 -0.028380  0.157293 -0.273941  0.275957  0.136321  0.229726   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8183  0.231076 -0.074579  0.161141 -0.239554  0.166832  0.142751  0.275325   \n",
       "8184  0.183570  0.064570  0.119314 -0.220391  0.185930  0.059797  0.320332   \n",
       "8185  0.265107  0.035628  0.237405 -0.208801  0.326406  0.151469  0.338959   \n",
       "8186  0.194329 -0.162931  0.175191 -0.272689  0.295109  0.161218  0.429787   \n",
       "8187  0.341165  0.044748  0.248957 -0.215969  0.217901  0.206946  0.298251   \n",
       "\n",
       "         7_ind     8_ind     9_ind  ...    30_cla    31_cla    32_cla  \\\n",
       "0     0.277950 -0.172449 -0.078276  ...  0.190652 -0.317788 -0.006029   \n",
       "1     0.121672 -0.082587  0.207627  ...  0.190652 -0.317788 -0.006029   \n",
       "2     0.057844 -0.132149 -0.147851  ...  0.190652 -0.317788 -0.006029   \n",
       "3     0.171259 -0.045913 -0.147986  ...  0.190652 -0.317788 -0.006029   \n",
       "4     0.066404 -0.059985  0.030648  ...  0.190652 -0.317788 -0.006029   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8183  0.102900 -0.062771 -0.113664  ... -0.297430  0.169880  0.206478   \n",
       "8184  0.277950 -0.172449 -0.078276  ... -0.297430  0.169880  0.206478   \n",
       "8185  0.051302 -0.042855  0.286972  ... -0.297430  0.169880  0.206478   \n",
       "8186  0.174258 -0.076592 -0.058829  ... -0.297430  0.169880  0.206478   \n",
       "8187  0.171259 -0.045913 -0.147986  ... -0.297430  0.169880  0.206478   \n",
       "\n",
       "        33_cla    34_cla    35_cla    36_cla    37_cla    38_cla    39_cla  \n",
       "0    -0.285176  0.176120 -0.070169 -0.269090 -0.445585 -0.012556  0.332329  \n",
       "1    -0.285176  0.176120 -0.070169 -0.269090 -0.445585 -0.012556  0.332329  \n",
       "2    -0.285176  0.176120 -0.070169 -0.269090 -0.445585 -0.012556  0.332329  \n",
       "3    -0.285176  0.176120 -0.070169 -0.269090 -0.445585 -0.012556  0.332329  \n",
       "4    -0.285176  0.176120 -0.070169 -0.269090 -0.445585 -0.012556  0.332329  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "8183 -0.285225 -0.196411 -0.136789 -0.248808 -0.360837 -0.111585  0.350478  \n",
       "8184 -0.285225 -0.196411 -0.136789 -0.248808 -0.360837 -0.111585  0.350478  \n",
       "8185 -0.285225 -0.196411 -0.136789 -0.248808 -0.360837 -0.111585  0.350478  \n",
       "8186 -0.285225 -0.196411 -0.136789 -0.248808 -0.360837 -0.111585  0.350478  \n",
       "8187 -0.285225 -0.196411 -0.136789 -0.248808 -0.360837 -0.111585  0.350478  \n",
       "\n",
       "[8188 rows x 80 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # X = adc[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', 'Homeothermic', 'HasEggs', 'HasMilk', 'HasGills', 'Eel', 'Ostrich', 'Trout', 'Crocodile', 'Girl', 'Lizard', 'Shark', 'Bat', 'Cat', 'Dolphin', 'Eagle', 'Dog', 'Snake', 'Herring', 'Boy', 'Turtle', 'Penguin', 'T-Rex', 'Platypus', 'Dragon']]\n",
    "# # X = adc[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39']]\n",
    "X = atomic_data[['0_ind', '1_ind', '2_ind', '3_ind', '4_ind', '5_ind', '6_ind', '7_ind', '8_ind', '9_ind', '10_ind', '11_ind', '12_ind', '13_ind', '14_ind', '15_ind', '16_ind', '17_ind', '18_ind', '19_ind', '20_ind', '21_ind', '22_ind', '23_ind', '24_ind', '25_ind', '26_ind', '27_ind', '28_ind', '29_ind', '30_ind', '31_ind', '32_ind', '33_ind', '34_ind', '35_ind', '36_ind', '37_ind', '38_ind', '39_ind', '0_cla', '1_cla', '2_cla', '3_cla', '4_cla', '5_cla', '6_cla', '7_cla', '8_cla', '9_cla', '10_cla', '11_cla', '12_cla', '13_cla', '14_cla', '15_cla', '16_cla', '17_cla', '18_cla', '19_cla', '20_cla', '21_cla', '22_cla', '23_cla', '24_cla', '25_cla', '26_cla', '27_cla', '28_cla', '29_cla', '30_cla', '31_cla', '32_cla', '33_cla', '34_cla', '35_cla', '36_cla', '37_cla', '38_cla', '39_cla']]\n",
    "y = atomic_data['Example Encoding']\n",
    "X#.shape\n",
    "# # y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X.values, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11864/693892417.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
      "/tmp/ipykernel_11864/693892417.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
      "/tmp/ipykernel_11864/693892417.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
      "/tmp/ipykernel_11864/693892417.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=200)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "X_train_tensor = torch.reshape(X_train_tensor, (X_train_tensor.shape[0], 1, X_train_tensor.shape[1]))\n",
    "X_test_tensor = torch.reshape(X_test_tensor, (X_test_tensor.shape[0], 1, X_test_tensor.shape[1]))\n",
    "y_train_tensor = torch.reshape(y_train_tensor, (y_train_tensor.shape[0], 1, y_train_tensor.shape[1]))\n",
    "y_test_tensor = torch.reshape(y_test_tensor, (y_test_tensor.shape[0], 1, y_test_tensor.shape[1]))\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 60\n",
    "output_size = 2 # num_classes = 2\n",
    "num_layers = 2\n",
    "learning_rate = 0.1\n",
    "num_epochs = 1000\n",
    "batch_size = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape torch.Size([6550, 1, 80]) torch.Size([6550, 1, 1])\n",
      "Testing Shape torch.Size([1638, 1, 80]) torch.Size([1638, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# X_train_tensor.shape\n",
    "print(\"Training Shape\", X_train_tensor.shape, y_train_tensor.shape)\n",
    "print(\"Testing Shape\", X_test_tensor.shape, y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ip503zyC0iN-"
   },
   "outputs": [],
   "source": [
    "# Define LSTM classifier model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, output_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.6228062510490417\n",
      "Epoch [2/100], Loss: 0.16640840470790863\n",
      "Epoch [3/100], Loss: 0.4871169924736023\n",
      "Epoch [4/100], Loss: 0.20392249524593353\n",
      "Epoch [5/100], Loss: 0.29082393646240234\n",
      "Epoch [6/100], Loss: 0.3220677375793457\n",
      "Epoch [7/100], Loss: 0.3209717571735382\n",
      "Epoch [8/100], Loss: 0.30023491382598877\n",
      "Epoch [9/100], Loss: 0.2686639428138733\n",
      "Epoch [10/100], Loss: 0.2312115877866745\n",
      "Epoch [11/100], Loss: 0.1926918625831604\n",
      "Epoch [12/100], Loss: 0.16329750418663025\n",
      "Epoch [13/100], Loss: 0.16851529479026794\n",
      "Epoch [14/100], Loss: 0.2126440405845642\n",
      "Epoch [15/100], Loss: 0.19914445281028748\n",
      "Epoch [16/100], Loss: 0.16685256361961365\n",
      "Epoch [17/100], Loss: 0.15947577357292175\n",
      "Epoch [18/100], Loss: 0.16612544655799866\n",
      "Epoch [19/100], Loss: 0.1748855859041214\n",
      "Epoch [20/100], Loss: 0.18061931431293488\n",
      "Epoch [21/100], Loss: 0.18185220658779144\n",
      "Epoch [22/100], Loss: 0.17882482707500458\n",
      "Epoch [23/100], Loss: 0.17278078198432922\n",
      "Epoch [24/100], Loss: 0.16578395664691925\n",
      "Epoch [25/100], Loss: 0.16058506071567535\n",
      "Epoch [26/100], Loss: 0.1598801612854004\n",
      "Epoch [27/100], Loss: 0.16402371227741241\n",
      "Epoch [28/100], Loss: 0.16861295700073242\n",
      "Epoch [29/100], Loss: 0.16822025179862976\n",
      "Epoch [30/100], Loss: 0.16371671855449677\n",
      "Epoch [31/100], Loss: 0.15995164215564728\n",
      "Epoch [32/100], Loss: 0.1592341512441635\n",
      "Epoch [33/100], Loss: 0.16066409647464752\n",
      "Epoch [34/100], Loss: 0.1624777615070343\n",
      "Epoch [35/100], Loss: 0.16345764696598053\n",
      "Epoch [36/100], Loss: 0.1631753146648407\n",
      "Epoch [37/100], Loss: 0.16185133159160614\n",
      "Epoch [38/100], Loss: 0.1601555347442627\n",
      "Epoch [39/100], Loss: 0.15892870724201202\n",
      "Epoch [40/100], Loss: 0.1587565541267395\n",
      "Epoch [41/100], Loss: 0.15951572358608246\n",
      "Epoch [42/100], Loss: 0.16032108664512634\n",
      "Epoch [43/100], Loss: 0.1602255403995514\n",
      "Epoch [44/100], Loss: 0.15910856425762177\n",
      "Epoch [45/100], Loss: 0.15767121315002441\n",
      "Epoch [46/100], Loss: 0.15661315619945526\n",
      "Epoch [47/100], Loss: 0.15603914856910706\n",
      "Epoch [48/100], Loss: 0.15553496778011322\n",
      "Epoch [49/100], Loss: 0.1546098291873932\n",
      "Epoch [50/100], Loss: 0.1530388444662094\n",
      "Epoch [51/100], Loss: 0.15080997347831726\n",
      "Epoch [52/100], Loss: 0.1479153037071228\n",
      "Epoch [53/100], Loss: 0.14457076787948608\n",
      "Epoch [54/100], Loss: 0.14151819050312042\n",
      "Epoch [55/100], Loss: 0.13849413394927979\n",
      "Epoch [56/100], Loss: 0.13412007689476013\n",
      "Epoch [57/100], Loss: 0.12829925119876862\n",
      "Epoch [58/100], Loss: 0.12232650816440582\n",
      "Epoch [59/100], Loss: 0.1167600005865097\n",
      "Epoch [60/100], Loss: 0.11075641959905624\n",
      "Epoch [61/100], Loss: 0.10417953133583069\n",
      "Epoch [62/100], Loss: 0.09787838160991669\n",
      "Epoch [63/100], Loss: 0.09224288165569305\n",
      "Epoch [64/100], Loss: 0.08685680478811264\n",
      "Epoch [65/100], Loss: 0.08185365051031113\n",
      "Epoch [66/100], Loss: 0.07763636857271194\n",
      "Epoch [67/100], Loss: 0.07393267750740051\n",
      "Epoch [68/100], Loss: 0.07065506279468536\n",
      "Epoch [69/100], Loss: 0.06776329129934311\n",
      "Epoch [70/100], Loss: 0.06545636802911758\n",
      "Epoch [71/100], Loss: 0.06295204907655716\n",
      "Epoch [72/100], Loss: 0.06065508723258972\n",
      "Epoch [73/100], Loss: 0.05939939618110657\n",
      "Epoch [74/100], Loss: 0.05739596486091614\n",
      "Epoch [75/100], Loss: 0.05592721328139305\n",
      "Epoch [76/100], Loss: 0.054050102829933167\n",
      "Epoch [77/100], Loss: 0.052069291472435\n",
      "Epoch [78/100], Loss: 0.05080879479646683\n",
      "Epoch [79/100], Loss: 0.04898896440863609\n",
      "Epoch [80/100], Loss: 0.0480758398771286\n",
      "Epoch [81/100], Loss: 0.04673774540424347\n",
      "Epoch [82/100], Loss: 0.045097410678863525\n",
      "Epoch [83/100], Loss: 0.04394974932074547\n",
      "Epoch [84/100], Loss: 0.0429363027215004\n",
      "Epoch [85/100], Loss: 0.042323917150497437\n",
      "Epoch [86/100], Loss: 0.042163487523794174\n",
      "Epoch [87/100], Loss: 0.04165397956967354\n",
      "Epoch [88/100], Loss: 0.040819261223077774\n",
      "Epoch [89/100], Loss: 0.040155161172151566\n",
      "Epoch [90/100], Loss: 0.04003433510661125\n",
      "Epoch [91/100], Loss: 0.03964386135339737\n",
      "Epoch [92/100], Loss: 0.03894531726837158\n",
      "Epoch [93/100], Loss: 0.03871561214327812\n",
      "Epoch [94/100], Loss: 0.03832698613405228\n",
      "Epoch [95/100], Loss: 0.03764541819691658\n",
      "Epoch [96/100], Loss: 0.03732416406273842\n",
      "Epoch [97/100], Loss: 0.03682689741253853\n",
      "Epoch [98/100], Loss: 0.03631088510155678\n",
      "Epoch [99/100], Loss: 0.03603333234786987\n",
      "Epoch [100/100], Loss: 0.035447828471660614\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, torch.flatten(y_train_tensor))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item())) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = model(X_test_tensor)\n",
    "data_predict = test_predict.data.numpy()\n",
    "dataY_plot = Y_test_tensor.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Expression          Individual       Example Type\n",
      "0         Cat       animals#cat01  positive examples\n",
      "1         Cat       animals#boy01  negative examples\n",
      "2         Cat    animals#lizard01  negative examples\n",
      "3         Cat     animals#snake01  negative examples\n",
      "4         Cat     animals#trout01  negative examples\n",
      "5         Cat       animals#bat01  negative examples\n",
      "6         Cat     animals#eagle01  negative examples\n",
      "7         Cat      animals#trex01  negative examples\n",
      "8         Cat   animals#ostrich01  negative examples\n",
      "9         Cat       animals#eel01  negative examples\n",
      "10        Cat       animals#dog01  negative examples\n",
      "11        Cat    animals#dragon01  negative examples\n",
      "12        Cat      animals#girl01  negative examples\n",
      "13        Cat    animals#turtle01  negative examples\n",
      "14        Cat     animals#shark01  negative examples\n",
      "15        Cat   animals#dolphin01  negative examples\n",
      "16        Cat   animals#herring01  negative examples\n",
      "17        Cat     animals#croco01  negative examples\n",
      "18        Cat  animals#platypus01  negative examples\n",
      "19        Cat   animals#penguin01  negative examples\n",
      "['positive examples' 'negative examples']\n",
      "(205976, 3)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LSTMClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLSTMClassifier\u001b[49m(input_size, hidden_size, num_layers, output_size, batch_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LSTMClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "model = LSTMClassifier(input_size, hidden_size, num_layers, output_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = model(X_test_tensor)\n",
    "loss = criterion(y_prediction, torch.flatten(y_test_tensor))\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_prediction = model(X_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_prediction.argmax(1) == y_test_tensor.squeeze()).to(float).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_test_tensor == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_prediction.argmax(1).numpy(), y_test_tensor.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_train_prediction.argmax(1).numpy(), y_train_tensor.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_prediction.argmax(1).numpy(), y_test_tensor.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
